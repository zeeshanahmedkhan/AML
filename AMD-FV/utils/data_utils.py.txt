import os
import json
import pickle
from typing import Dict, List, Tuple, Optional
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
import logging
from pathlib import Path
import shutil
import random
from tqdm import tqdm

class DatasetPreprocessor:
    """Utility class for dataset preprocessing and organization"""
    
    def __init__(self, 
                 raw_data_dir: Path,
                 processed_data_dir: Path,
                 train_ratio: float = 0.8,
                 val_ratio: float = 0.1,
                 min_images_per_identity: int = 5):
        
        self.raw_data_dir = Path(raw_data_dir)
        self.processed_data_dir = Path(processed_data_dir)
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.min_images_per_identity = min_images_per_identity
        
        # Create necessary directories
        self.processed_data_dir.mkdir(parents=True, exist_ok=True)
        (self.processed_data_dir / 'train').mkdir(exist_ok=True)
        (self.processed_data_dir / 'val').mkdir(exist_ok=True)
        (self.processed_data_dir / 'test').mkdir(exist_ok=True)

    def organize_dataset(self) -> Dict[str, int]:
        """Organize raw dataset into train/val/test splits"""
        # Collect all identities and their images
        identities = {}
        for identity in os.listdir(self.raw_data_dir):
            identity_dir = self.raw_data_dir / identity
            if identity_dir.is_dir():
                images = list(identity_dir.glob('*.jpg')) + \
                        list(identity_dir.glob('*.png'))
                if len(images) >= self.min_images_per_identity:
                    identities[identity] = images

        # Split identities into train/val/test
        all_identities = list(identities.keys())
        random.shuffle(all_identities)
        
        n_total = len(all_identities)
        n_train = int(n_total * self.train_ratio)
        n_val = int(n_total * self.val_ratio)
        
        train_identities = all_identities[:n_train]
        val_identities = all_identities[n_train:n_train + n_val]
        test_identities = all_identities[n_train + n_val:]

        # Copy files to respective directories
        splits = {'train': train_identities, 
                 'val': val_identities, 
                 'test': test_identities}
        
        stats = {}
        for split, split_identities in splits.items():
            split_dir = self.processed_data_dir / split
            n_images = self._copy_identity_images(split_dir, 
                                                split_identities, 
                                                identities)
            stats[split] = n_images

        return stats

    def _copy_identity_images(self,
                            split_dir: Path,
                            identities: List[str],
                            identity_images: Dict[str, List[Path]]) -> int:
        """Copy images for given identities to split directory"""
        n_images = 0
        for identity in tqdm(identities, desc=f"Processing {split_dir.name}"):
            identity_dir = split_dir / identity
            identity_dir.mkdir(exist_ok=True)
            
            for img_path in identity_images[identity]:
                shutil.copy2(img_path, identity_dir / img_path.name)
                n_images += 1
                
        return n_images

    def generate_pairs(self, 
                      split: str,
                      n_positive: int,
                      n_negative: int) -> List[Tuple[Path, Path, int]]:
        """Generate positive and negative pairs for verification"""
        split_dir = self.processed_data_dir / split
        identities = [d for d in split_dir.iterdir() if d.is_dir()]
        
        pairs = []
        
        # Generate positive pairs
        for _ in range(n_positive):
            identity = random.choice(identities)
            images = list(identity.glob('*.jpg')) + list(identity.glob('*.png'))
            if len(images) >= 2:
                img1, img2 = random.sample(images, 2)
                pairs.append((img1, img2, 1))
                
        # Generate negative pairs
        for _ in range(n_negative):
            identity1, identity2 = random.sample(identities, 2)
            img1 = random.choice(list(identity1.glob('*.jpg')) + 
                               list(identity1.glob('*.png')))
            img2 = random.choice(list(identity2.glob('*.jpg')) + 
                               list(identity2.glob('*.png')))
            pairs.append((img1, img2, 0))
            
        random.shuffle(pairs)
        return pairs

class FaceVerificationDataset(Dataset):
    """Dataset class for face verification pairs"""
    
    def __init__(self,
                 pairs: List[Tuple[Path, Path, int]],
                 transform=None):
        self.pairs = pairs
        self.transform = transform

    def __len__(self) -> int:
        return len(self.pairs)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:
        img1_path, img2_path, label = self.pairs[idx]
        
        img1 = Image.open(img1_path).convert('RGB')
        img2 = Image.open(img2_path).convert('RGB')
        
        if self.transform:
            img1 = self.transform(img1)
            img2 = self.transform(img2)
            
        return img1, img2, label

def create_verification_dataloaders(
        pairs_train: List[Tuple[Path, Path, int]],
        pairs_val: List[Tuple[Path, Path, int]],
        transform_train,
        transform_val,
        batch_size: int,
        num_workers: int) -> Tuple[DataLoader, DataLoader]:
    """Create dataloaders for verification task"""
    
    train_dataset = FaceVerificationDataset(pairs_train, transform_train)
    val_dataset = FaceVerificationDataset(pairs_val, transform_val)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader